{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Improved Knowledge Distillation via Teacher Assistant**\n",
    "\n",
    "#### **Abstract**\n",
    "- 当老师太强，学生太弱时，学生网络的性能会降低。**这就造成了限制，我们不能对于任意大的 *teacher model*，都找到一个很小的 *student model* 进行蒸馏**，为解决这一问题，作者引入了 *Teacher Assistant*（助教）来弥补学生和老师之间的差距\n",
    "- 数据集：*CIFAR-10*、*CIFAR-100*、*ImageNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "#### **Introduction**\n",
    "- 前人工作：*Model compression techniques have emerged to address such issues, e.g., parameter pruning and sharing (Han, Mao, and Dally 2016), low-rank factorization (Tai et al. 2015) and knowledge distillation (Bucila, Caruana, and Niculescu-Mizil 2006; Hinton, Vinyals, and Dean 2015)*\n",
    "- **KD 并不是总是有效，当 teacher 和 student 的差距过大时，结果就差强人意**。这个应该是模型容量的问题\n",
    "- 为解决这个问题，作者提出，在 teacher 和 student 给定的情况下，应该有一个合适的 *assistant*，来帮助 *student* 学习\n",
    "\n",
    "> **这里，*teacher, student, assistant* 取多大，是一个很重要的问题，详细请见原文实验**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "#### **Related Work**\n",
    "- *Model Compression*\n",
    "    - *There has been an interesting line of research that compresses a large network by reducing the connections based on **weight magnitudes** (Han, Mao, and Dally 2016; Li et al. 2016) or **importance scores** (Yu et al. 2018). The reduced network is **fine-tuned** on the same dataset to retain its accuracy*\n",
    "    - 一言以蔽之，**Network Pruning**，然后再 **Fine-tune**\n",
    "- *Knowledge Distillation*\n",
    "    - 与各种提出的架构不同，**作者是考虑 teacher 和 student 现在都是确定的情况下，如何提升 student 的性能**\n",
    "- *Distillation Theory*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "#### **Assistant based Knowledge Distillation**\n",
    "- **原文显示，*student* 并不会随着 *teacher* 的增强而增强，是一个先增强后减弱的过程**\n",
    "\n",
    "- 架构图如图所示\n",
    "\n",
    "![Figure 1: TA fills the gap between student & teacher](../img/Figure-1_TAKD.png)\n",
    "\n",
    "> 一言以蔽之，**作者提出了一个 *Teacher Assistant*，来帮助 *student* 学习**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "#### **Others**\n",
    "\n",
    "- [code](https://github.com/imirzadeh/Teacher-Assistant-KnowledgeDistillation)，作者已删除仓库\n",
    "\n",
    "> **具体实验效果，可视化分析，理论分析见原文**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
