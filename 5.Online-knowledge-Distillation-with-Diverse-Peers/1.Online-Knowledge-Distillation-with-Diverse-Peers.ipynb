{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Online Knowledge Distillation with Diverse Peers**\n",
    "\n",
    "#### **Abstract**\n",
    "\n",
    "- KD 并不总是可用\n",
    "- **组派生**的目标(*group-derived targets*)为无教师蒸馏提供了良好的配方，但组成员通过简单的聚合函数**快速均质(*homogenized*)，导致早期饱和**\n",
    "- *Online Knowledge Distillation with Diverse peers(OKDDip), which performs two-level distillation during training with multiple auxiliary peers and one group leader.*\n",
    "    - **In the first-level distillation**, each auxiliary peer holds an individual set of **aggregation weights generated with an attention-based mechanism** to derive its own targets from predictions of other auxiliary peers.\n",
    "    - **The second-level distillation** is performed to transfer the knowledge in the ensemble of auxiliary peers further **to the group leader**, i.e., the model used for inference\n",
    "- 在不牺牲训练和推理性能的情况下，**OKDDip 效果比最先进的方法还要好**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "#### **Introduction**\n",
    "- Since the quality of predictions varies among peers, **it is important to treat peers unequally** (Lan, Zhu, and Gong 2018).\n",
    "  - Unfortunately, naive aggregation functions tend to **cause peers to quickly homogenize, hurting the effectiveness of group distillation** (Kuncheva and Whitaker 2003; Zhou 2012).\n",
    "\n",
    "- Unlike naive group-based learning where all peers end up with similar behaviors, **trained peer models in our approach could be quite different from each other**.\n",
    "\n",
    "- **A key design** of OKDDip is that each auxiliary peer assigns individual weights to all the peers during aggregation **to derive their own target distributions**. We incorporate an **attention-based mechanism** (Vaswani et al. 2017) to generate a distinct set of weights **for each peer** to measure the importance of group members. This allows large variation in derived target distributions and hence boosts peer diversity. **Note that asymmetric weights exist in our model**.\n",
    "\n",
    "- dataset: CIFAR-10, CIFAR-100, ImageNet-2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "#### **Related Work**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
