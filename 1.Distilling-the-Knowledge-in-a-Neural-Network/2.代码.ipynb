{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_softmax(input_tensor, temperature=1.0):\n",
    "    return torch.softmax(input_tensor / temperature, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = model.TeacherModel()\n",
    "student = model.Student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Student(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear1): Linear(in_features=784, out_features=1000, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear2): Linear(in_features=1000, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "student.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = data_loader.load_data_MNIST(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move models and data to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher = teacher.to(device)\n",
    "student = student.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion_teacher = nn.CrossEntropyLoss()\n",
    "criterion_student = nn.CrossEntropyLoss()\n",
    "optimizer_teacher = optim.RMSprop(teacher.parameters(), lr=1e-4)\n",
    "optimizer_student = optim.SGD(student.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define teacher evaluation function\n",
    "def evaluate_teacher(net, criterion, testloader, device):\n",
    "    net.eval()\t# Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in testloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            y_hat = temperature_softmax(net(X),) \n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = y_hat.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "    print(f\"Test loss = {running_loss / len(testloader)}, accuracy = {correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define teacher training function\n",
    "def train_teacher(net, optimizer, criterion, trainloader, testloader, device, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\t# Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        for X, y in trainloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # y_hat = net(X)\n",
    "            y_hat = temperature_softmax(net(X))\n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, loss = {running_loss / len(trainloader)}\")\n",
    "        if epoch % 5 == 5 - 1:\n",
    "            evaluate_teacher(net, criterion, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 1.4996710412045742\n",
      "Epoch 2, loss = 1.4985767963084768\n",
      "Epoch 3, loss = 1.497438501804433\n",
      "Epoch 4, loss = 1.497542148955325\n",
      "Epoch 5, loss = 1.4983872002743661\n",
      "Test loss = 1.4822357922792435, accuracy = 0.9794\n",
      "Epoch 6, loss = 1.4960177030969173\n",
      "Epoch 7, loss = 1.4951664777512246\n",
      "Epoch 8, loss = 1.4951716321579953\n",
      "Epoch 9, loss = 1.4950845895929539\n",
      "Epoch 10, loss = 1.4941560106074556\n",
      "Test loss = 1.4808611243963241, accuracy = 0.9804\n",
      "Epoch 11, loss = 1.4930315550337447\n",
      "Epoch 12, loss = 1.4934779796194524\n",
      "Epoch 13, loss = 1.4928328534390063\n",
      "Epoch 14, loss = 1.4922175057390903\n",
      "Epoch 15, loss = 1.492471792342815\n",
      "Test loss = 1.479246386885643, accuracy = 0.9821\n",
      "Epoch 16, loss = 1.4908364174213815\n",
      "Epoch 17, loss = 1.4913386162291182\n",
      "Epoch 18, loss = 1.491286019061474\n",
      "Epoch 19, loss = 1.4890501057848018\n",
      "Epoch 20, loss = 1.4898812745479828\n",
      "Test loss = 1.478385517001152, accuracy = 0.9825\n"
     ]
    }
   ],
   "source": [
    "train_teacher(teacher, optimizer_teacher, criterion_teacher, train_iter, test_iter, device,num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define student evaluation function\n",
    "def evaluate_student(net, criterion, testloader, device):\n",
    "    net.eval()\t# Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in testloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # y_hat = net(X)\n",
    "            y_hat = temperature_softmax(net(X))\n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = y_hat.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "    print(f\"Test loss = {running_loss / len(testloader)}, accuracy = {correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define student training function\n",
    "def train_student(net, optimizer, criterion, trainloader,testloader, device, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\t# Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        for X, y in trainloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # y_hat = net(X)\n",
    "            y_hat = temperature_softmax(net(X))\n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, loss = {running_loss / len(trainloader)}\")\n",
    "        if epoch % 5 == 5 - 1:\n",
    "            evaluate_student(net, criterion, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 1.2536832418847592\n",
      "Epoch 2, loss = 0.8337080724695896\n",
      "Epoch 3, loss = 0.7318104987448835\n",
      "Epoch 4, loss = 0.6757268994412524\n",
      "Epoch 5, loss = 0.6392246447979135\n",
      "Test loss = 0.5993871130049229, accuracy = 0.798\n",
      "Epoch 6, loss = 0.6138397891470726\n",
      "Epoch 7, loss = 0.5910799625072073\n",
      "Epoch 8, loss = 0.5727639648508518\n",
      "Epoch 9, loss = 0.5575131676298507\n",
      "Epoch 10, loss = 0.5464943954285155\n",
      "Test loss = 0.5272816389799118, accuracy = 0.8224\n"
     ]
    }
   ],
   "source": [
    "train_student(student, optimizer_student, criterion_student, train_iter, test_iter, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distill(teacher, student, optimizer_student, criterion_student, trainloader, testloader, device, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        teacher.eval()\n",
    "        student.train()\n",
    "        running_loss = 0.0\n",
    "        for X, y in trainloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_teacher = teacher(X)\n",
    "            y_hat = student(X)\n",
    "            loss = 5 * criterion_student(y_hat, y) + F.kl_div(y_hat.log(), y_teacher, reduction=\"batchmean\")\n",
    "            \n",
    "            optimizer_student.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_student.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, loss = {running_loss / len(trainloader)}\")\n",
    "        if epoch % 5 == 5 - 1:\n",
    "            evaluate_student(student, criterion_student, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_distill(teacher, student, optimizer_student, criterion_student, trainloader, testloader, device, num_epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainloader' is not defined"
     ]
    }
   ],
   "source": [
    "train_distill(teacher, student, optimizer_student, criterion_student, train_iter, test_iter, device, num_epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
