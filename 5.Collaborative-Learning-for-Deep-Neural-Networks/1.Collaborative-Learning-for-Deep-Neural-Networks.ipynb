{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Collaborative Learning for Deep Neural Networks**\n",
    "\n",
    "#### **Abstract**\n",
    "\n",
    "- 引入了协作学习方法，其中**同一网络的多个分类器头同时**在相同的训练数据上进行训练。\n",
    "- 协作学习方法融合了多种学习方法的优点，包括辅助训练、多任务学习和知识蒸馏。**不增加额外的推断成本。**\n",
    "- 协作学习的**两个关键机制：**\n",
    "   - 第一，多个分类器头对相同示例的多个视图一致性提供了额外信息，**同时对每个分类器进行了正则化**，以提高泛化能力。\n",
    "   - 第二，通过中间级表示（ILR）共享和反向传播重新缩放，**汇总了来自所有头部的梯度流**，降低了训练的计算复杂性，**同时有助于监督共享层。**\n",
    "\n",
    "- 数据集：*CIFAR, ImageNet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "#### **Introduction**\n",
    "- *The per-layer network weight distribution **shows that** ILR sharing reduces the number of \"dead\" filter weights in the bottom layers **due to** the vanishing gradient issue, thereby enlarging the network capacity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "#### **Collaborative learning**\n",
    "\n",
    "##### **Generation of training graph**\n",
    "\n",
    "![Figure 1: Multiple head patterns for training. Three colors represent subnets g1, g2, and g3 in (1).](../img/Figure-1_Multiple-head-patterns-for-training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **loss**\n",
    "- **每个 *head* 含有两部分 *loss***\n",
    "  - 和 *ground-truth* 的交叉熵\n",
    "  - 将其余的 *head* 的 *logit* 进行相加求均值，和 *head* 的 *logit* 进行交叉熵\n",
    "  - 这两部分损失被一个**相对权重**平衡，**且和为 1**\n",
    "\n",
    "详见原文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Optimization for a group of classifier heads**\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{h=1}^{H} L^{(h)} + \\lambda \\Omega(\\mathbf{\\theta})$$ \n",
    "\n",
    "**也就是说，同时对所有的 *head* 进行更新，并进行正则化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation rescaling**\n",
    "\n",
    "![Figure 2: No rescaling vs backpropagation rescaling](../img/Figure-2_No-rescaling-vs-backpropagation-rescaling.png)\n",
    "\n",
    "> **这里存在着反向梯度分配的问题**\n",
    "\n",
    "*Therefore, backpropagation rescaling is proposed to **achieve two goals at the same time** – to normalize the backpropagation flow in subnet $g_1$ and keep that in subnet $g_2$ the same as the single classifier case. The solution to add a new operation $I(·)$ between $g_1$ and $g_2$, shown in Figure 2 (b), which is*\n",
    "\n",
    "$$\\mathcal{I}(\\mathbf{x}) = \\mathbf{x}, \\quad \\nabla_x I = \\frac{1}{H}$$\n",
    "\n",
    "*And then the backpropagation input for the shared layers becomes*\n",
    "$$\\nabla_{x_1} L = \\frac{1}{H} \\sum_{h=1}^{H} \\nabla_{x_1} L^{(h)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "#### **Experiments**\n",
    "- 代码使用 *TensorFlow* 实现，详见原文\n",
    "- 具体**共享层**详见原文"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
